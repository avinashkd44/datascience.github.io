<!DOCTYPE html>
<!-- Created by pdf2htmlEX (https://github.com/coolwanglu/pdf2htmlex) -->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta charset="utf-8"/>

<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
<link rel="stylesheet" href="base.min.css"/>
<link rel="stylesheet" href="fancy.min.css"/>
<link rel="stylesheet" href="main.css"/>
<script src="compatibility.min.js"></script>
<script src="theViewer.min.js"></script>
<script>
try{
theViewer.defaultViewer = new theViewer.Viewer({});
}catch(e){}
</script>
<title></title>
</head>
<body>
<div id="sidebar">
<div id="outline">
</div>
</div>
<div id="page-container">
<div id="pf1" class="pf w0 h0" data-page-no="1"><div class="pc pc1 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg1.png"/><div class="t m0 x1 h1 y1 ff1 fs0 fc0 sc0 ls0 ws0">LINEAR REGRESSION</div><div class="t m0 x2 h2 y2 ff2 fs1 fc0 sc0 ls0 ws0">Avinash Dwivedi</div><div class="t m0 x2 h2 y3 ff2 fs1 fc0 sc0 ls0 ws0">AI ML DL NLP Cloud Consultant</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf2" class="pf w0 h0" data-page-no="2"><div class="pc pc2 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg2.png"/><div class="t m0 x3 h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">OBJECTIVE</div><div class="t m0 x4 h4 y5 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Intro to Linear regression model</span></div><div class="t m0 x4 h4 y6 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Advantage &amp; Disadvantage of Linear regression</span></div><div class="t m0 x4 h4 y7 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Cost function and Gradient Descent</span></div><div class="t m0 x4 h4 y8 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Role of gradient Descent in interpretation of model</span></div><div class="t m0 x4 h4 y9 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Difference between R squared &amp; adjusted  R squared</span></div><div class="t m0 x4 h4 ya ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Assumption made to build linear regression model</span></div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf3" class="pf w0 h0" data-page-no="3"><div class="pc pc3 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg3.png"/><div class="t m0 x5 h3 y4 ff1 fs2 fc0 sc0 ls0 ws0"> REGRESSION?</div><div class="t m0 x4 h4 y5 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Regression is a method of modelling a target value based on independent </span></div><div class="t m0 x6 h4 yb ff1 fs4 fc0 sc0 ls0 ws0">predictors.</div><div class="t m0 x4 h4 yc ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">method is mostly used for forecasting and finding out cause and effect </span></div><div class="t m0 x6 h4 yd ff1 fs4 fc0 sc0 ls0 ws0">relationship between variables</div><div class="t m0 x4 h4 ye ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Regression techniques mostly differ based on the number of independent </span></div><div class="t m0 x6 h4 yf ff1 fs4 fc0 sc0 ls0 ws0">variables and the type of relationship between the independent and </div><div class="t m0 x6 h4 y10 ff1 fs4 fc0 sc0 ls0 ws0">dependent variables</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf4" class="pf w0 h0" data-page-no="4"><div class="pc pc4 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg4.png"/><div class="t m0 x7 h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">LINEAR REGRESSION???</div><div class="t m0 x8 h4 y11 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Linear regression performs the task to predict a dependent variable value (y) </span></div><div class="t m0 x9 h4 y12 ff1 fs4 fc0 sc0 ls0 ws0">based on a given independent variable (x). So, this regression technique </div><div class="t m0 x9 h4 y13 ff1 fs4 fc0 sc0 ls0 ws0">finds out a linear relationship between x (input) and y(output). Hence, the </div><div class="t m0 x9 h4 y14 ff1 fs4 fc0 sc0 ls0 ws0">name is Linear Regression.</div><div class="t m0 x9 h4 y15 ff1 fs4 fc0 sc0 ls0 ws0">In the figure above, X (input) is the work experience and Y (output) is the </div><div class="t m0 x9 h4 y16 ff1 fs4 fc0 sc0 ls0 ws0">salary of a person. The regression line is the best fit line for our model..</div><div class="t m0 x8 h5 y17 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">y = a_0 + a_1 * x <span class="_ _1"></span>     ---<span class="ff4"></span> Linear Equation</span></div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf5" class="pf w0 h0" data-page-no="5"><div class="pc pc5 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg5.png"/><div class="t m0 xa h3 y18 ff2 fs2 fc0 sc0 ls0 ws0">HYPOTHESIS FUNCTION</div><div class="t m0 xb h4 y19 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff2 fs4">y = a_0 + a_1 * x</span></div><div class="t m0 xb h4 y1a ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">While training the model we are given :</span></div><div class="t m0 xc h4 y1b ff2 fs4 fc0 sc0 ls0 ws0">x:<span class="ff1">input training data (univariate – one input variable(parameter))</span></div><div class="t m0 xc h4 y1c ff2 fs4 fc0 sc0 ls0 ws0">y:<span class="ff1">labels to data (supervised learning)</span></div><div class="t m0 xb h4 y1d ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">When training the model – it fits the best line to predict the value of y for a </span></div><div class="t m0 xc h4 y1e ff1 fs4 fc0 sc0 ls0 ws0">given value of x. The model gets the best regression fit line by finding the best </div><div class="t m0 xc h4 y1f ff2 fs4 fc0 sc0 ls0 ws0">a_0 and a_1<span class="ff1">values.</span></div><div class="t m0 xc h4 y20 ff2 fs4 fc0 sc0 ls0 ws0">a_0 :<span class="ff1">intercept</span></div><div class="t m0 xc h4 y21 ff2 fs4 fc0 sc0 ls0 ws0">a_1 :<span class="ff1">coefficient of x</span></div><div class="t m0 xb h4 y22 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Once we find the best <span class="ff2">a_0 and a_1 </span>values, we get the best fit line. So when </span></div><div class="t m0 xc h4 y23 ff1 fs4 fc0 sc0 ls0 ws0">we are finally using our model for prediction, it will predict the value of y for </div><div class="t m0 xc h4 y24 ff1 fs4 fc0 sc0 ls0 ws0">the input value of x.</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf6" class="pf w0 h0" data-page-no="6"><div class="pc pc6 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg6.png"/><div class="t m0 xd h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">COST FUNCTION</div><div class="t m0 x4 h4 y5 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">COST FUNCTION:-</span></div><div class="t m0 x4 h4 y6 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">The different values for weights or coefficient of lines (a</span></div><div class="t m0 xe h6 y25 ff1 fs5 fc0 sc0 ls0 ws0">0</div><div class="t m0 xf h4 y6 ff1 fs4 fc0 sc0 ls0 ws0">, a</div><div class="t m0 x10 h6 y25 ff1 fs5 fc0 sc0 ls0 ws0">1</div><div class="t m0 x11 h4 y6 ff1 fs4 fc0 sc0 ls0 ws0">) gives the </div><div class="t m0 x6 h4 yc ff1 fs4 fc0 sc0 ls0 ws0">different line of regression, and the cost function is used to estimate the </div><div class="t m0 x6 h4 yd ff1 fs4 fc0 sc0 ls0 ws0">values of the coefficient for the best fit line<span class="_ _1"></span>.</div><div class="t m0 x4 h4 ye ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Cost function optimizes the regression coefficients or weights. It measures </span></div><div class="t m0 x6 h4 yf ff1 fs4 fc0 sc0 ls0 ws0">how a linear regression model is performing.</div><div class="t m0 x4 h4 y26 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">We can use the cost function to find the accuracy of the<span class="_ _1"></span><span class="ff2">mapping function<span class="_ _1"></span></span>, </span></div><div class="t m0 x6 h4 y27 ff1 fs4 fc0 sc0 ls0 ws0">which maps the input variable to the output variable. This mapping function </div><div class="t m0 x6 h4 y28 ff1 fs4 fc0 sc0 ls0 ws0">is also known as<span class="_ _2"></span><span class="ff2">Hypothesis function<span class="_ _1"></span><span class="ff1">.</span></span></div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf7" class="pf w0 h0" data-page-no="7"><div class="pc pc7 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg7.png"/><div class="t m0 x4 h4 y5 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Want the best values for a_0 and <span class="_ _1"></span>a_1???</span></div><div class="t m0 x4 h4 y6 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">By achieving the best-fit regression line, the model aims to predict y value </span></div><div class="t m0 x6 h4 yc ff1 fs4 fc0 sc0 ls0 ws0">such that the error difference between predicted value and true value is </div><div class="t m0 x6 h4 yd ff1 fs4 fc0 sc0 ls0 ws0">minimum.</div><div class="t m0 x4 h4 ye ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Cost function(J) of Linear Regression is the<span class="ff2">Root Mean Squared Error </span></span></div><div class="t m0 x6 h4 yf ff2 fs4 fc0 sc0 ls0 ws0">(RMSE)<span class="ff1">between predicted y value (pred) and true y value (y<span class="_ _1"></span>).</span></div><div class="t m0 x4 h4 y26 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff2 fs4">SOLUTION<span class="ff1">:----</span></span></div><div class="t m0 x4 h4 y29 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">convert this search problem into a minimization problem where we would </span></div><div class="t m0 x6 h4 y2a ff1 fs4 fc0 sc0 ls0 ws0">like to minimize the error between the predicted value and the actual value<span class="_ _1"></span>.</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf8" class="pf w0 h0" data-page-no="8"><div class="pc pc8 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg8.png"/><div class="t m0 x12 h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">MEAN SQUARE ERROR</div><div class="t m0 x4 h4 y5 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">The difference between the predicted values and ground truth measures </span></div><div class="t m0 x6 h4 yb ff1 fs4 fc0 sc0 ls0 ws0">the error difference. We square the error difference and sum over all data </div><div class="t m0 x6 h4 y2b ff1 fs4 fc0 sc0 ls0 ws0">points and divide that value by the total number of data points. This provides </div><div class="t m0 x6 h4 y2c ff1 fs4 fc0 sc0 ls0 ws0">the average squared error over all the data points. Therefore, this cost </div><div class="t m0 x6 h4 y2d ff1 fs4 fc0 sc0 ls0 ws0">function is also known as the Mean Squared Error(MSE) function<span class="_ _1"></span>.</div><div class="t m0 x4 h4 y2e ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">using this MSE function we are going to change the values of a_0 and a_1 </span></div><div class="t m0 x6 h4 y2f ff1 fs4 fc0 sc0 ls0 ws0">such that the MSE value settles at the minima</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf9" class="pf w0 h0" data-page-no="9"><div class="pc pc9 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg9.png"/><div class="t m0 x13 h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">GRADIENT DESCENT</div><div class="t m0 x4 h4 y30 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Gradient descent is used to minimize the MSE by calculating the gradient of </span></div><div class="t m0 x6 h4 y31 ff1 fs4 fc0 sc0 ls0 ws0">the cost function.</div><div class="t m0 x4 h4 y32 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">To update a_0 and a_1 values in order to reduce Cost function (minimizing </span></div><div class="t m0 x6 h4 y33 ff1 fs4 fc0 sc0 ls0 ws0">RMSE value) and achieving the best fit line the model uses Gradient Descent. </div><div class="t m0 x6 h4 y34 ff1 fs4 fc0 sc0 ls0 ws0">The idea is to start with random θ</div><div class="t m0 x14 h6 y35 ff1 fs5 fc0 sc0 ls0 ws0">1</div><div class="t m0 x15 h4 y34 ff1 fs4 fc0 sc0 ls0 ws0">and θ</div><div class="t m0 x16 h6 y35 ff1 fs5 fc0 sc0 ls0 ws0">2</div><div class="t m0 x17 h4 y34 ff1 fs4 fc0 sc0 ls0 ws0">values and then iteratively </div><div class="t m0 x6 h4 y36 ff1 fs4 fc0 sc0 ls0 ws0">updating the values, reaching minimum cost.</div><div class="t m0 x4 h4 y37 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">A regression model uses gradient descent to update the coefficients of the </span></div><div class="t m0 x6 h4 y38 ff1 fs4 fc0 sc0 ls0 ws0">line by reducing the cost function.</div><div class="t m0 x4 h4 y39 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">It is done by a random selection of values of coefficient and then iteratively </span></div><div class="t m0 x6 h4 y3a ff1 fs4 fc0 sc0 ls0 ws0">update the values to reach the minimum cost function<span class="_ _1"></span>.</div><div class="t m0 x4 h4 y3b ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">You may be wondering how to use gradient descent to update a_0 and </span></div><div class="t m0 x6 h4 y3c ff1 fs4 fc0 sc0 ls0 ws0">a_1??</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pfa" class="pf w0 h0" data-page-no="a"><div class="pc pca w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bga.png"/><div class="t m0 x4 h2 y3d ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">To update a_0 and a_1, we take gradients from the cost function. To find these </span></div><div class="t m0 x6 h2 y3e ff1 fs1 fc0 sc0 ls0 ws0">gradients, we take partial derivatives with respect to a_0 and a_1.</div><div class="t m0 x4 h2 y3f ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">The partial derivative are the gradients and they are used to update the values of </span></div><div class="t m0 x6 h2 y40 ff1 fs1 fc0 sc0 ls0 ws0">a_0 and a_1. </div><div class="t m0 x4 h2 y41 ff1 fs1 fc0 sc0 ls0 ws0">Alpha is the learning rate which is a hyper parameter .</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pfb" class="pf w0 h0" data-page-no="b"><div class="pc pcb w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bgb.png"/><div class="t m0 x14 h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">QUESTION AND ANSWER</div><div class="t m0 x4 h2 y3d ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">1. For Feature selection what are the different techniques available and how to </span></div><div class="t m0 x6 h2 y3e ff1 fs1 fc0 sc0 ls0 ws0">apply in regression problem?</div><div class="t m0 x4 h2 y3f ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">Answer – </span></div><div class="t m0 x4 h2 y42 ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">Feature selection refers to techniques that select a subset of the most relevant </span></div><div class="t m0 x6 h2 y43 ff1 fs1 fc0 sc0 ls0 ws0">features (columns) for a dataset. Fewer features can allow machine learning </div><div class="t m0 x6 h2 y44 ff1 fs1 fc0 sc0 ls0 ws0">algorithms to run more efficiently (less space or time complexity) and be more </div><div class="t m0 x6 h2 y45 ff1 fs1 fc0 sc0 ls0 ws0">effective. Some machine learning algorithms can be misled by irrelevant input </div><div class="t m0 x6 h2 y46 ff1 fs1 fc0 sc0 ls0 ws0">features, resulting in worse predictive performance<span class="_ _1"></span>.</div><div class="t m0 x4 h2 y47 ff1 fs1 fc0 sc0 ls0 ws0">     There are three approach  to do feature selection.</div><div class="t m0 x18 h7 y48 ff3 fs7 fc0 sc0 ls0 ws0">•<span class="_ _4"> </span><span class="ff1 fs8">1. Manual – Manually analyze it and chose the best feature and eliminate others.</span></div><div class="t m0 x18 h7 y49 ff3 fs7 fc0 sc0 ls0 ws0">•<span class="_ _4"> </span><span class="ff1 fs8">2. Using Algorithm  -&gt; RFE ,AIC ,BIC(This also for model selection) etc</span></div><div class="t m0 x18 h7 y4a ff3 fs7 fc0 sc0 ls0 ws0">•<span class="_ _4"> </span><span class="ff1 fs8">3. Mixed approach –&gt; Algorithm + Manual</span></div><div class="t m0 x4 h2 y4b ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">RFE -&gt; <span class="ff2">Recursive Feature Elimination – Best way for now</span></span></div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pfc" class="pf w0 h0" data-page-no="c"><div class="pc pcc w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bgc.png"/><div class="t m0 x19 h3 y4c ff1 fs2 fc0 sc0 ls0 ws0">RFE -&gt; <span class="_ _5"></span><span class="ff2">RECURSIVE FEATURE </span></div><div class="t m0 x1a h3 y4d ff2 fs2 fc0 sc0 ls0 ws0">ELIMINATION</div><div class="t m0 x4 h2 y3d ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">RFE is a wrapper-type feature selection algorithm. This means that a different </span></div><div class="t m0 x6 h2 y3e ff1 fs1 fc0 sc0 ls0 ws0">machine learning algorithm is given and used in the core of the method, is wrapped </div><div class="t m0 x6 h2 y4e ff1 fs1 fc0 sc0 ls0 ws0">by RFE, and used to help select features. </div><div class="t m0 x4 h2 y40 ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">This is in contrast to filter-based feature selections that score each feature and select </span></div><div class="t m0 x6 h2 y4f ff1 fs1 fc0 sc0 ls0 ws0">those features with the largest (or smallest) score.</div><div class="t m0 x4 h2 y50 ff1 fs1 fc0 sc0 ls0 ws0">For detail understanding just go through this blog and read RFE for regression- </div><div class="t m0 x4 h2 y51 ff1 fs1 fc1 sc0 ls0 ws0">https://machinelearningmastery.com/rfe-feature-selection-in-python<span class="_ _6"></span>/</div><div class="t m0 x4 h2 y52 ff1 fs1 fc0 sc0 ls0 ws0">For AIC BIC or other algorithm read below-</div><div class="t m0 x4 h2 y53 ff1 fs1 fc1 sc0 ls0 ws0">https://www.methodology.psu.edu/resources/AIC-vs-BIC<span class="_ _5"></span>/</div><div class="t m0 x4 h2 y54 ff1 fs1 fc1 sc0 ls0 ws0">https://www.hindawi.com/journals/js/2015/142612<span class="_ _1"></span>/</div><div class="t m0 x4 h2 y55 ff1 fs1 fc1 sc0 ls0 ws0">https://machinelearningmastery.com/probabilistic-model-selection-measures/</div><a class="l" href="https://machinelearningmastery.com/rfe-feature-selection-in-python/"><div class="d m1" style="border-style:none;position:absolute;left:61.200012px;bottom:179.144562px;width:665.595703px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://machinelearningmastery.com/rfe-feature-selection-in-python/"><div class="d m1" style="border-style:none;position:absolute;left:726.950012px;bottom:179.144562px;width:8.740234px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://www.methodology.psu.edu/resources/AIC-vs-BIC/"><div class="d m1" style="border-style:none;position:absolute;left:61.200012px;bottom:120.744537px;width:550.175781px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://www.methodology.psu.edu/resources/AIC-vs-BIC/"><div class="d m1" style="border-style:none;position:absolute;left:611.325012px;bottom:120.744537px;width:8.740234px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://www.hindawi.com/journals/js/2015/142612/"><div class="d m1" style="border-style:none;position:absolute;left:61.200012px;bottom:91.544525px;width:482.880859px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://www.hindawi.com/journals/js/2015/142612/"><div class="d m1" style="border-style:none;position:absolute;left:544.450012px;bottom:91.544525px;width:8.740234px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://machinelearningmastery.com/probabilistic-model-selection-measures/"><div class="d m1" style="border-style:none;position:absolute;left:61.200012px;bottom:62.344513px;width:758.554688px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pfd" class="pf w0 h0" data-page-no="d"><div class="pc pcd w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bgd.png"/><div class="t m0 x14 h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">QUESTION AND ANSWER</div><div class="t m0 x4 h4 y5 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">2 Normalization Vs standardization in case of feature selection in regression?</span></div><div class="t m0 x4 h4 y6 ff1 fs4 fc0 sc0 ls0 ws0">  Answer  </div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pfe" class="pf w0 h0" data-page-no="e"><div class="pc pce w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bge.png"/><div class="t m0 x17 h3 y56 ff2 fs2 fc0 sc0 ls0 ws0">NORMALIZATION VS </div><div class="t m0 x1b h3 y4d ff2 fs2 fc0 sc0 ls0 ws0">STANDARDIZATION</div><div class="t m0 x4 h2 y57 ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff2 fs1">Normalization<span class="ff1">is a good technique to use when you do not know the distribution of </span></span></div><div class="t m0 x6 h2 y58 ff1 fs1 fc0 sc0 ls0 ws0">your data or when you know the distribution is not Gaussian (a bell curve). </div><div class="t m0 x6 h2 y59 ff1 fs1 fc0 sc0 ls0 ws0">Normalization is useful when your data has varying scales and the algorithm you are </div><div class="t m0 x6 h2 y5a ff1 fs1 fc0 sc0 ls0 ws0">using does not make assumptions about the distribution of your data, such as k-</div><div class="t m0 x6 h2 y5b ff1 fs1 fc0 sc0 ls0 ws0">nearest neighbors and artificial neural networks.</div><div class="t m0 x4 h2 y5c ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff2 fs1">Standardization<span class="ff1">assumes that your data has a Gaussian (bell curve) distribution. This </span></span></div><div class="t m0 x6 h2 y5d ff1 fs1 fc0 sc0 ls0 ws0">does not strictly have to be true, but the technique is more effective if your attribute </div><div class="t m0 x6 h2 y5e ff1 fs1 fc0 sc0 ls0 ws0">distribution is Gaussian. Standardization is useful when your data has varying scales </div><div class="t m0 x6 h2 y5f ff1 fs1 fc0 sc0 ls0 ws0">and the algorithm you are using does make assumptions about your data having a </div><div class="t m0 x6 h2 y60 ff1 fs1 fc0 sc0 ls0 ws0">Gaussian distribution, such as linear regression, logistic regression, and linear </div><div class="t m0 x6 h2 y61 ff1 fs1 fc0 sc0 ls0 ws0">discriminant analysis.</div><div class="t m0 x4 h2 y62 ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">For more detail - <span class="fc1">https://towardsai.net/p/data-science/how-when-and-why-should-</span></span></div><div class="t m0 x6 h2 y63 ff1 fs1 fc1 sc0 ls0 ws0">you-normalize-standardize-rescale-your-data-3f083def38ff</div><a class="l" href="https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"><div class="d m1" style="border-style:none;position:absolute;left:242.699997px;bottom:83.988129px;width:644.287125px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"><div class="d m1" style="border-style:none;position:absolute;left:79.200012px;bottom:62.388123px;width:560.527344px;height:23.828125px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pff" class="pf w0 h0" data-page-no="f"><div class="pc pcf w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bgf.png"/><div class="t m0 x1c h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">QNA</div><div class="t m0 x4 h4 y5 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">3. Difference between Bias and Variance?</span></div><div class="t m0 x4 h4 y6 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Answer -&gt;</span></div><div class="t m0 x4 h4 y7 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Bias is the simplifying assumptions made by the model to make the target </span></div><div class="t m0 x6 h4 y64 ff1 fs4 fc0 sc0 ls0 ws0">function easier to approximate.</div><div class="t m0 x4 h4 y65 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Variance is the amount that the estimate of the target function will change </span></div><div class="t m0 x6 h4 y66 ff1 fs4 fc0 sc0 ls0 ws0">given different training data.</div><div class="t m0 x4 h4 y67 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Go through it - <span class="fc1">https://machinelearningmastery.com/gentle-introduction-to-</span></span></div><div class="t m0 x6 h4 y29 ff1 fs4 fc1 sc0 ls0 ws0">the-bias-variance-trade-off-in-machine-</div><div class="t m0 x6 h4 y2a ff1 fs4 fc1 sc0 ls0 ws0">learning/#:~:text=Bias%20is%20the%20simplifying%20assumptions,change%20</div><div class="t m0 x6 h4 y68 ff1 fs4 fc1 sc0 ls0 ws0">given%20different%20training%20data.</div><a class="l" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Bias%2520is%2520the%2520simplifying%2520assumptions,change%2520given%2520different%2520training%2520data."><div class="d m1" style="border-style:none;position:absolute;left:239.699997px;bottom:156.826965px;width:649.472672px;height:26.210938px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Bias%2520is%2520the%2520simplifying%2520assumptions,change%2520given%2520different%2520training%2520data."><div class="d m1" style="border-style:none;position:absolute;left:79.200012px;bottom:133.066956px;width:424.445312px;height:26.210938px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Bias%2520is%2520the%2520simplifying%2520assumptions,change%2520given%2520different%2520training%2520data."><div class="d m1" style="border-style:none;position:absolute;left:79.200012px;bottom:109.306946px;width:815.697266px;height:26.210938px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Bias%2520is%2520the%2520simplifying%2520assumptions,change%2520given%2520different%2520training%2520data."><div class="d m1" style="border-style:none;position:absolute;left:79.200012px;bottom:85.546936px;width:411.006836px;height:26.210938px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf10" class="pf w0 h0" data-page-no="10"><div class="pc pc10 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg10.png"/><div class="t m0 x1c h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">QNA</div><div class="t m0 x4 h2 y69 ff3 fs6 fc0 sc0 ls0 ws0">•<span class="_ _3"> </span><span class="ff1 fs1">4. What is Constant Variance of Error term?</span></div><div class="t m0 x4 h2 y6a ff1 fs1 fc0 sc0 ls0 ws0">Answer -&gt; </div><div class="t m0 x4 h2 y6b ff1 fs1 fc0 sc0 ls0 ws0">it means that the distribution of error terms will </div><div class="t m0 x4 h2 y6c ff1 fs1 fc0 sc0 ls0 ws0">have the same “spread” at every </div><div class="t m0 x4 h2 y6d ff1 fs1 fc0 sc0 ls0 ws0">possiblexvalue on the regression line. </div><div class="t m0 x4 h2 y6e ff1 fs1 fc0 sc0 ls0 ws0">It means that when you plot the individual </div><div class="t m0 x4 h2 y6f ff1 fs1 fc0 sc0 ls0 ws0">error against the predicted value, the </div><div class="t m0 x4 h2 y70 ff1 fs1 fc0 sc0 ls0 ws0">variance of the error predicted value should </div><div class="t m0 x4 h2 y71 ff1 fs1 fc0 sc0 ls0 ws0">be constant. See the red arrows in the picture </div><div class="t m0 x4 h2 y72 ff1 fs1 fc0 sc0 ls0 ws0">below, the length of the red lines (a proxy of </div><div class="t m0 x4 h2 y73 ff1 fs1 fc0 sc0 ls0 ws0">its variance) are the same.</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf11" class="pf w0 h0" data-page-no="11"><div class="pc pc11 w0 h0"><img class="bi x0 y0 w0 h0" alt="" src="bg11.png"/><div class="t m0 x1c h3 y4 ff1 fs2 fc0 sc0 ls0 ws0">QNA</div><div class="t m0 x4 h4 y5 ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">5. Can we drop some variables after analyzing EDA before RFE or keep it </span></div><div class="t m0 x6 h4 yb ff1 fs4 fc0 sc0 ls0 ws0">and let RFE run using all the columns?</div><div class="t m0 x4 h4 yc ff3 fs3 fc0 sc0 ls0 ws0">•<span class="_ _0"> </span><span class="ff1 fs4">Answer – Both way will work and possible.</span></div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
</div>
<div class="loading-indicator">

</div>
</body>
</html>
